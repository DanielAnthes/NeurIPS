# Notes

## Loss

- loss scales with lookahead, should it be normalized?
- there are large spikes in the loss when training with Adam, is this related to the optimizer?

## Reward

- reward becomes unstable when training for too long
